from pathlib import Path
from typing import TypedDict, Dict, Any, List
from datetime import datetime
from langgraph.graph import StateGraph, END
import json
import re
from rank_bm25 import BM25Okapi

from ulanggraph.workflow_core import WorkflowBase
from ulanggraph.file_processor import FileProcessor
from ulanggraph.data_processorllm import DataProcessor
from extrfinetune.cftm import FineTunedModelProcessor
from extrfinetune.ctotable import ElsevierTableExtractor
from extrfinetune.cjtj import CrystalDataComparator
from extrfinetune.chl import AcronymExtractor
from extrfinetune.cstrucout import MOFDataProcessor 

class StateSchema(TypedDict):
    """å·¥ä½œæµçŠ¶æ€æ¨¡å¼"""
    current_step: str
    timestamp: str
    data: Dict[str, Any]
    file_paths: Dict[str, Path]

class MOFWorkflowManager(WorkflowBase):
    """MOFå·¥ä½œæµç®¡ç†å™¨"""
    def __init__(self, config_path: str, output_dir: str):
        super().__init__(config_path, output_dir)
        self.file_processor = FileProcessor(config_path, output_dir)
        self.data_processor = DataProcessor(config_path, output_dir)
        # Add structure directory to data_processor's output_dirs
        structure_dir = self.data_processor.output_dirs['final'] / 'structure'
        structure_dir.mkdir(exist_ok=True)
        self.data_processor.output_dirs['structure'] = structure_dir


    def process_text_with_bm25(self, text: str, abbreviations: Dict) -> str:
        """
        ä½¿ç”¨BM25ç®—æ³•å¤„ç†æ–‡æœ¬ä¸­çš„ç¼©å†™
        
        Args:
            text: éœ€è¦å¤„ç†çš„æ–‡æœ¬
            abbreviations: ç¼©å†™è¯å…¸æ•°æ®
        
        Returns:
            å¤„ç†åçš„æ–‡æœ¬
        """
        # å®šä¹‰ç¼©å†™æ¨¡å¼
        patterns = [
            r'\b(?:H\d*L\d*)\b',    # åŒ¹é… HL, H2L, HL2 ç­‰
            r'\b(?:L\d+)\b',        # åŒ¹é… L1, L2 ç­‰
            r'\b(?:L\d+H\d+)\b',    # åŒ¹é… L1H1, L2H2 ç­‰
            r'\bL\b'                # åŒ¹é…å•ä¸ª L
        ]
        
        # æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦å­˜åœ¨åŒ¹é…çš„æ¨¡å¼
        has_patterns = any(re.search(pattern, text, re.IGNORECASE) for pattern in patterns)
        if not has_patterns:
            return text
                
        # æå–æ‰€æœ‰å¯èƒ½çš„ç¼©å†™
        all_abbreviations = []
        for identifier, abbr_list in abbreviations.items():
            for abbr_info in abbr_list:
                abbr = abbr_info['abbreviation'].replace('Abbreviation: ', '')
                full_name = abbr_info['full_name'].replace('Full Name: ', '')
                all_abbreviations.append((abbr, full_name))
        
        if not all_abbreviations:
            return text
                
        # åˆ›å»ºBM25è¯­æ–™åº“
        corpus = [abbr for abbr, _ in all_abbreviations]
        bm25 = BM25Okapi([list(abbr.lower()) for abbr in corpus])
        
        # å¤„ç†æ–‡æœ¬ä¸­çš„æ¯ä¸ªåŒ¹é…é¡¹
        processed_text = text
        found_replacements = []
        
        for pattern in patterns:
            matches = list(re.finditer(pattern, processed_text))
            for match in reversed(matches):  # ä»åå‘å‰å¤„ç†ï¼Œé¿å…æ›¿æ¢ä½ç½®æ”¹å˜
                abbr = match.group()
                # ä½¿ç”¨BM25æœç´¢æœ€åŒ¹é…çš„ç¼©å†™
                query = list(abbr.lower())
                scores = bm25.get_scores(query)
                
                # å°†numpyæ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ‰¾åˆ°æœ€é«˜åˆ†çš„ç´¢å¼•
                scores_list = scores.tolist()
                if scores_list:
                    max_score = max(scores_list)
                    if max_score > 0.5:  # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼
                        best_match_idx = scores_list.index(max_score)
                        original_abbr, full_name = all_abbreviations[best_match_idx]
                        
                        # åªæœ‰å½“ç¼©å†™å®Œå…¨åŒ¹é…æ—¶æ‰æ·»åŠ åˆ°æ›¿æ¢åˆ—è¡¨
                        if abbr.upper() == original_abbr.upper():
                            found_replacements.append((abbr, full_name))
        
        # æ ¼å¼åŒ–è¾“å‡º
        if found_replacements:
            # ç§»é™¤ç°æœ‰çš„ç¼©å†™æ³¨é‡Šï¼ˆå¦‚æœæœ‰ï¼‰
            lines = processed_text.split('\n')
            cleaned_lines = [line for line in lines 
                        if not (line.startswith('Abbreviation:') or 
                                line.startswith('Full Name:') or 
                                line.startswith('No abbreviations'))]
            
            # æ·»åŠ æ–°çš„ç¼©å†™æ³¨é‡Š
            for abbr, full_name in sorted(set(found_replacements)):
                cleaned_lines.append(f"Abbreviation: {abbr}")
                cleaned_lines.append(f"Full Name: {full_name}")
                
            return '\n'.join(cleaned_lines)
        
        return text

    def process_synthesis(self, state: StateSchema) -> StateSchema:
        """å¤„ç†åˆæˆæ•°æ®"""
        print("\nğŸ”„ Processing synthesis data...")
        processor = FineTunedModelProcessor(
            config_path=str(self.config_path),
            test_folder=str(state['file_paths']['input_dir']),
            system_file_path=str(state['file_paths']['system_file']),
            output_folder=str(self.data_processor.output_dirs['synthesis'])
        )
        
        results = processor.run()
        
        synthesis_files = list(self.data_processor.output_dirs['synthesis'].glob(
            f"testpredictions_shot198_{self.timestamp}*.xlsx"))
        
        if not synthesis_files:
            raise RuntimeError("No synthesis output file found")
            
        output_path = max(synthesis_files, key=lambda p: p.stat().st_mtime)
        print(f"âœ… Synthesis processing complete. Output saved to: {output_path}")
        
        state['file_paths']['synthesis_output'] = output_path
        state['current_step'] = "extract_tables"
        return state

    def extract_tables(self, state: StateSchema) -> StateSchema:
        """æå–è¡¨æ ¼æ•°æ®"""
        print("\nğŸ”„ Extracting tables...")
        output_file = self.data_processor.output_dirs['tables'] / f"tables_{self.timestamp}.json"
        
        extractor = ElsevierTableExtractor(
            config_path=str(self.config_path),
            input_folder=str(state['file_paths']['input_dir']),
            output_file=str(output_file)
        )
        
        results = extractor.run()
        state['file_paths']['tables_output'] = output_file
        print(f"âœ… Table extraction complete. Output saved to: {output_file}")
        
        state['current_step'] = "compare_data"
        return state

    def compare_data(self, state: StateSchema) -> StateSchema:
        """æ¯”è¾ƒæ•°æ®"""
        print("\nğŸ”„ Comparing data...")
        comparator = CrystalDataComparator(config_path=str(self.config_path))
        output_file = self.data_processor.output_dirs['comparison'] / f"comparison_{self.timestamp}.json"
        
        comparator.process(
            str(state['file_paths']['ccdc_data']),
            str(state['file_paths']['tables_output']),
            str(output_file)
        )
        
        state['file_paths']['comparison_output'] = output_file
        print(f"âœ… Data comparison complete. Output saved to: {output_file}")
        
        state['current_step'] = "process_abbreviations"
        return state

    def process_abbreviations(self, state: StateSchema) -> StateSchema:
        """å¤„ç†ç¼©å†™"""
        print("\nğŸ”„ Processing abbreviations...")
        try:
            # è·å–åˆæˆæ•°æ®
            synthesis_data = self.file_processor.get_synthesis_data(
                state['file_paths']['synthesis_output']
            )
            
            # è°ƒç”¨ç¼©å†™æå–å™¨
            extractor = AcronymExtractor(
                config_path=str(self.config_path),
                input_folder=str(state['file_paths']['input_dir']),
                output_folder=str(self.data_processor.output_dirs['final'])
            )
            
            # è¿è¡Œæå–å™¨å¹¶è·å–ç»“æœ
            raw_results = extractor.run()
            
            # æ£€æŸ¥è¿”å›å€¼ç±»å‹
            if isinstance(raw_results, str):
                print(f"âš ï¸ Unexpected result type from extractor: {raw_results}")
                # å°è¯•åŠ è½½ä¿å­˜çš„JSONæ–‡ä»¶
                acronym_files = list(self.data_processor.output_dirs['final'].glob('acronym_results_*.json'))
                if acronym_files:
                    latest_file = max(acronym_files, key=lambda p: p.stat().st_mtime)
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        raw_results = json.load(f)
                    print(f"âœ… Successfully loaded results from {latest_file}")
                else:
                    print("âŒ No acronym files found")
                    state['current_step'] = "generate_final_output"
                    return state
            
            # å¤„ç†å’Œæ ¼å¼åŒ–ç»“æœ
            formatted_results = {}
            if raw_results and isinstance(raw_results, dict):
                for identifier, abbr_list in raw_results.items():
                    if identifier and abbr_list:
                        # ç¡®ä¿æ¯ä¸ªç¼©å†™éƒ½æœ‰æ­£ç¡®çš„æ ¼å¼
                        formatted_abbrs = []
                        for abbr in abbr_list:
                            if isinstance(abbr, dict):
                                # å¦‚æœå·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼ŒéªŒè¯å¹¶æ ‡å‡†åŒ–æ ¼å¼
                                abbreviation = abbr.get('abbreviation', '')
                                full_name = abbr.get('full_name', '')
                                if abbreviation and not abbreviation.startswith('Abbreviation: '):
                                    abbreviation = f"Abbreviation: {abbreviation}"
                                if full_name and not full_name.startswith('Full Name: '):
                                    full_name = f"Full Name: {full_name}"
                                formatted_abbrs.append({
                                    'abbreviation': abbreviation,
                                    'full_name': full_name
                                })
                            elif isinstance(abbr, (list, tuple)) and len(abbr) == 2:
                                # å¦‚æœæ˜¯å…ƒç»„æˆ–åˆ—è¡¨æ ¼å¼ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                                formatted_abbrs.append({
                                    'abbreviation': f"Abbreviation: {abbr[0]}",
                                    'full_name': f"Full Name: {abbr[1]}"
                                })
                        
                        if formatted_abbrs:
                            formatted_results[identifier] = formatted_abbrs
            
            # ä¿å­˜æ ¼å¼åŒ–åçš„ç»“æœ
            if formatted_results:
                output_file = self.data_processor.output_dirs['final'] / f"acronym_results_{int(datetime.now().timestamp())}.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(formatted_results, f, indent=4, ensure_ascii=False)
                
                state['data']['abbreviations'] = formatted_results
                print(f"âœ… Processed {len(formatted_results)} compounds with abbreviations")
                print(f"ğŸ’¾ Abbreviations saved to: {output_file}")
            else:
                print("âš ï¸ No valid abbreviations were extracted")
            
            state['current_step'] = "generate_final_output"
            return state
            
        except Exception as e:
            print(f"âŒ Error in abbreviation processing: {str(e)}")
            if 'abbreviations' not in state['data']:
                state['data']['abbreviations'] = {}
            state['current_step'] = "generate_final_output"
            return state

    def generate_final_output(self, state: StateSchema) -> StateSchema:
        """ç”Ÿæˆæœ€ç»ˆè¾“å‡º"""
        print("\nğŸ”„ Generating final output...")
        try:
            output_file = self.data_processor.output_dirs['final'] / f"final_output_{self.timestamp}.txt"
            
            comparison_data = self.file_processor.read_file(state['file_paths']['comparison_output'])
            synthesis_data = self.file_processor.get_synthesis_data(state['file_paths']['synthesis_output'])
            
            final_output = self.data_processor.format_final_output(
                comparison_data,
                synthesis_data,
                state['data'].get('abbreviations')
            )
            
            if final_output is None:
                final_output = "Error: No output generated"
            elif not isinstance(final_output, str):
                final_output = str(final_output)
                
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(final_output)
                
            state['file_paths']['final_output'] = output_file
            print(f"âœ… Final output generated and saved to: {output_file}")
            
            state['current_step'] = "post_process_final_output"
            return state
            
        except Exception as e:
            print(f"âŒ Error generating final output: {e}")
            error_output = self.data_processor.output_dirs['final'] / f"final_output_error_{self.timestamp}.txt"
            with open(error_output, 'w', encoding='utf-8') as f:
                f.write(f"Error occurred: {str(e)}")
            state['file_paths']['final_output'] = error_output
            state['current_step'] = "post_process_final_output"
            return state

    def post_process_final_output(self, state: StateSchema) -> StateSchema:
        """å¤„ç†æœ€ç»ˆè¾“å‡ºæ–‡ä»¶"""
        print("\nğŸ”„ Post-processing final output...")
        try:
            if 'final_output' not in state['file_paths']:
                print("âŒ No final output file found in state")
                state['current_step'] = END
                return state

            # åˆ›å»ºtxtå­æ–‡ä»¶å¤¹
            txt_output_dir = self.data_processor.output_dirs['final'] / 'txt'
            txt_output_dir.mkdir(exist_ok=True)
            print(f"ğŸ“ Created TXT output directory: {txt_output_dir}")

            # è¯»å–åŸå§‹è¾“å‡ºæ–‡ä»¶
            output_file = state['file_paths']['final_output']
            with open(output_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # è¯»å–ç¼©å†™æ•°æ®
            acronym_files = list(self.data_processor.output_dirs['final'].glob('acronym_results_*.json'))
            
            # åˆå§‹åŒ–processed_outputsåˆ—è¡¨
            processed_files = []

            # å¤„ç†æ¯ä¸ªæ®µè½å¹¶æŒ‰æ–‡ä»¶åˆ†ç»„
            sections = content.split('\n\n')
            file_sections = {}

            for section in sections:
                if not section.strip():
                    continue
                    
                # è·å–æ ‡è¯†ç¬¦ï¼ˆæ–‡ä»¶åï¼‰
                identifier = section.split('\n')[0].strip()

                # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å
                output_filename = f"{identifier}_{self.timestamp}.txt"
                output_path = txt_output_dir / output_filename
                
                # ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(section)
                
                processed_files.append(output_path)
                print(f"ğŸ’¾ Processed output for {identifier} saved to: {output_path}")

            # ä¿å­˜ä¸€ä¸ªæ±‡æ€»ä¿¡æ¯æ–‡ä»¶
            summary_file = self.data_processor.output_dirs['final'] / f"processing_summary_{self.timestamp}.txt"
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(f"Processing Summary\n")
                f.write(f"Timestamp: {self.timestamp}\n")
                f.write(f"Total files processed: {len(processed_files)}\n")
                f.write("\nProcessed Files:\n")
                for file in processed_files:
                    f.write(f"- {file.name}\n")

            # æ›´æ–°çŠ¶æ€
            state['file_paths']['processed_outputs'] = processed_files
            state['file_paths']['summary_file'] = summary_file
            
            print(f"ğŸ“ Generated {len(processed_files)} individual output files in {txt_output_dir}")
            print(f"ğŸ“‹ Summary saved to: {summary_file}")

            state['current_step'] = "process_to_structure"
            return state

        except Exception as e:
            print(f"âŒ Error in post processing: {e}")
            state['current_step'] = "process_to_structure"
            return state
            
    def process_to_structure(self, state: StateSchema) -> StateSchema:
        """Convert processed output files to structured Markdown format"""
        print("\nğŸ”„ Converting outputs to structured Markdown format...")
        try:
            if 'processed_outputs' not in state['file_paths'] or not state['file_paths']['processed_outputs']:
                print("âŒ No processed output files found")
                state['current_step'] = END
                return state

            # Get the directory containing processed txt files
            txt_dir = state['file_paths']['processed_outputs'][0].parent
            
            # Initialize MOF data processor
            mof_processor = MOFDataProcessor(str(self.config_path))
            
            # Set output markdown file path
            structure_output = self.data_processor.output_dirs['structure'] / f"structure_output_{self.timestamp}.md"
            
            try:
                print(f"ğŸ“ Processing files from: {txt_dir}")
                print(f"ğŸ“‚ Found {len(state['file_paths']['processed_outputs'])} files to process")
                
                mof_processor.process_directory(
                    str(txt_dir),
                    str(structure_output)
                )
                
                state['file_paths']['structure_output'] = structure_output
                print(f"âœ… Structure processing complete. Output saved to: {structure_output}")
                
            except Exception as e:
                print(f"âŒ Error during structure processing: {e}")
                raise
            
            state['current_step'] = END
            return state

        except Exception as e:
            print(f"âŒ Error in structure processing: {e}")
            state['current_step'] = END
            return state

    def create_workflow(self) -> StateGraph:
        """åˆ›å»ºå·¥ä½œæµå›¾"""
        workflow = StateGraph(state_schema=StateSchema)
        
        # Add all nodes
        workflow.add_node("process_synthesis", self.process_synthesis)
        workflow.add_node("extract_tables", self.extract_tables)
        workflow.add_node("compare_data", self.compare_data)
        workflow.add_node("process_abbreviations", self.process_abbreviations)
        workflow.add_node("generate_final_output", self.generate_final_output)
        workflow.add_node("post_process_final_output", self.post_process_final_output)
        workflow.add_node("process_to_structure", self.process_to_structure)  # Add new node
        
        # Set workflow sequence
        workflow.set_entry_point("process_synthesis")
        workflow.add_edge("process_synthesis", "extract_tables")
        workflow.add_edge("extract_tables", "compare_data")
        workflow.add_edge("compare_data", "process_abbreviations")
        workflow.add_edge("process_abbreviations", "generate_final_output")
        workflow.add_edge("generate_final_output", "post_process_final_output")
        workflow.add_edge("post_process_final_output", "process_to_structure")  # Add new edge
        workflow.add_edge("process_to_structure", END)
        
        return workflow.compile()

    def run(self, input_dir: str, system_file: str, ccdc_data: str):
        """è¿è¡Œå®Œæ•´å·¥ä½œæµ"""
        print("\nğŸš€ Starting MOF workflow...")
        
        initial_state: StateSchema = {
            "current_step": "process_synthesis",
            "timestamp": self.timestamp,
            "data": {},
            "file_paths": {
                'input_dir': Path(input_dir),
                'system_file': Path(system_file),
                'ccdc_data': Path(ccdc_data)
            }
        }
        
        workflow = self.create_workflow()
        try:
            final_state = workflow.invoke(initial_state)
            print(f"\nâœ¨ Workflow completed successfully!")
            print(f"ğŸ“„ Final output saved to: {final_state['file_paths']['final_output']}")
            return final_state
        except Exception as e:
            print(f"\nâŒ Workflow failed: {str(e)}")
            raise
